"""SWE-bench scenario example.

Demonstrates:
- Using SWE-bench Docker images for code repair tasks
- Creating WarmPool programmatically via Python SDK
- Applying code patches (simulating LLM agent behavior)
- Running test scripts to verify fixes
- Complete workflow for automated bug fixing scenarios
"""

from arl import SandboxSession, TaskStep, WarmPoolManager


def main() -> None:
    """Demonstrate SWE-bench scenario with code patching and testing."""
    print("=" * 60)
    print("Example: SWE-bench Scenario - Code Repair and Testing")
    print("=" * 60)

    # Configuration
    pool_name = "swebench-emotion"
    namespace = "default"
    swebench_image = "swebench/swesmith.x86_64.emotion_1776_js-emotion.b882bcba"

    # Step 0: Create WarmPool using Python SDK (no YAML required!)
    print("\n[Step 0] Setting up WarmPool with Python SDK...")
    print(f"Creating WarmPool '{pool_name}' with SWE-bench image...")

    warmpool_manager = WarmPoolManager(namespace=namespace)

    try:
        # Try to get existing warmpool
        warmpool_manager.get_warmpool(pool_name)
        print(f"✓ WarmPool '{pool_name}' already exists")
    except Exception:
        # Create new warmpool if it doesn't exist
        warmpool_manager.create_warmpool(
            name=pool_name,
            image=swebench_image,
            replicas=2,
            testbed_path="/testbed",  # SWE-bench uses /testbed directory
        )
        print(f"✓ WarmPool '{pool_name}' created")

        # Wait for warmpool to be ready
        print("Waiting for warm pods to be ready...")
        warmpool_manager.wait_for_warmpool_ready(pool_name)
        print("✓ WarmPool is ready with warm pods")

    with SandboxSession(pool_ref=pool_name, namespace=namespace, keep_alive=True) as session:
        print(f"\n✓ Sandbox allocated from pool '{pool_name}'")

        # Step 1: Inspect the environment
        print("\n[Step 1] Inspecting SWE-bench environment...")
        steps_inspect: list[TaskStep] = [
            {
                "name": "check_workspace",
                "type": "Command",
                "command": ["sh", "-c", "pwd && ls -la"],
                "workDir": "/testbed",
            },
            {
                "name": "check_repo",
                "type": "Command",
                "command": ["sh", "-c", "git status || echo 'Not a git repo'"],
                "workDir": "/testbed",
            },
        ]
        result_inspect = session.execute(steps_inspect)
        status_inspect = result_inspect.get("status", {})
        print(f"Environment check: {status_inspect.get('state')}")
        if status_inspect.get("stdout"):
            print(f"Workspace:\n{status_inspect.get('stdout')}")

        # Step 2: Apply a bug fix patch (simulating LLM agent)
        print("\n[Step 2] Applying code patch (simulating LLM agent fix)...")

        # Example patch: Fix a hypothetical bug in emotion library
        # NOTE: This is a simulated example. In real SWE-bench scenarios,
        # the patch would be generated by an LLM agent based on the actual
        # repository state. The line numbers and file structure here are
        # illustrative and may not match the actual SWE-bench environment.
        patch_content = """--- a/packages/emotion/src/index.js
+++ b/packages/emotion/src/index.js
@@ -10,7 +10,7 @@
   const styles = createStyles(props)

   // Apply styles to element
-  element.style = styles
+  element.setAttribute('style', styles)

   return element
 }
"""

        steps_patch: list[TaskStep] = [
            {
                "name": "create_patch_file",
                "type": "FilePatch",
                "path": "/testbed/fix.patch",
                "content": patch_content,
            },
            {
                "name": "show_original_file",
                "type": "Command",
                "command": [
                    "sh",
                    "-c",
                    "if [ -f packages/emotion/src/index.js ]; then "
                    "echo 'Original file:' && cat packages/emotion/src/index.js; "
                    "else echo 'File not found, creating demo structure'; fi",
                ],
                "workDir": "/testbed",
            },
            {
                "name": "create_demo_structure",
                "type": "Command",
                "command": [
                    "sh",
                    "-c",
                    "mkdir -p packages/emotion/src && "
                    "cat > packages/emotion/src/index.js << 'EOF'\n"
                    "export function applyEmotion(element, props) {\n"
                    "  const styles = createStyles(props)\n"
                    "  \n"
                    "  // Apply styles to element\n"
                    "  element.style = styles\n"
                    "  \n"
                    "  return element\n"
                    "}\n"
                    "EOF",
                ],
                "workDir": "/testbed",
            },
            {
                "name": "apply_patch",
                "type": "Command",
                "command": [
                    "sh",
                    "-c",
                    "patch -p1 < /testbed/fix.patch || echo 'Patch application completed'",
                ],
                "workDir": "/testbed",
            },
            {
                "name": "verify_patch",
                "type": "Command",
                "command": [
                    "sh",
                    "-c",
                    "echo 'Patched file:' && cat packages/emotion/src/index.js",
                ],
                "workDir": "/testbed",
            },
        ]

        result_patch = session.execute(steps_patch)
        status_patch = result_patch.get("status", {})
        print(f"Patch application: {status_patch.get('state')}")
        if status_patch.get("stdout"):
            print(f"Output:\n{status_patch.get('stdout')[:500]}...")

        # Step 3: Run test scripts to verify the fix
        print("\n[Step 3] Running test scripts to verify fix...")

        steps_test: list[TaskStep] = [
            {
                "name": "create_test_script",
                "type": "FilePatch",
                "path": "/testbed/run_tests.sh",
                "content": """#!/bin/bash
set -e

echo "Running test suite..."
echo "===================="

# Check if patched code is correct
if grep -q "setAttribute" packages/emotion/src/index.js; then
    echo "✓ Test 1: Patch applied correctly"
else
    echo "✗ Test 1: Patch not applied"
    exit 1
fi

# Simulate running actual tests
echo "✓ Test 2: Unit tests passed"
echo "✓ Test 3: Integration tests passed"
echo "✓ Test 4: Style application works correctly"

echo ""
echo "===================="
echo "All tests passed!"
exit 0
""",
            },
            {
                "name": "make_executable",
                "type": "Command",
                "command": ["chmod", "+x", "/testbed/run_tests.sh"],
                "workDir": "/testbed",
            },
            {
                "name": "run_tests",
                "type": "Command",
                "command": ["/bin/bash", "/testbed/run_tests.sh"],
                "workDir": "/testbed",
            },
        ]

        result_test = session.execute(steps_test)
        status_test = result_test.get("status", {})
        print(f"Test execution: {status_test.get('state')}")
        if status_test.get("stdout"):
            print(f"Test results:\n{status_test.get('stdout')}")

        # Step 4: Generate report
        print("\n[Step 4] Generating fix report...")

        steps_report: list[TaskStep] = [
            {
                "name": "create_report",
                "type": "FilePatch",
                "path": "/testbed/fix_report.md",
                "content": """# Bug Fix Report

## Issue
Incorrect style application in emotion library causing DOM manipulation errors.

## Root Cause
Direct assignment to `element.style` property instead of using `setAttribute()`.

## Fix Applied
Changed line 13 in packages/emotion/src/index.js:
- Before: `element.style = styles`
- After: `element.setAttribute('style', styles)`

## Testing
All test suites passed:
- ✓ Unit tests
- ✓ Integration tests
- ✓ Style application tests

## Verification
The fix has been verified in the SWE-bench environment and all tests pass.
""",
            },
            {
                "name": "show_report",
                "type": "Command",
                "command": ["cat", "/testbed/fix_report.md"],
                "workDir": "/testbed",
            },
        ]

        result_report = session.execute(steps_report)
        status_report = result_report.get("status", {})
        print(f"Report generation: {status_report.get('state')}")
        if status_report.get("stdout"):
            print(f"\n{status_report.get('stdout')}")

        # Summary
        print("\n" + "=" * 60)
        print("SWE-bench Scenario Completed Successfully!")
        print("=" * 60)
        print("\n✓ WarmPool created via Python SDK (no YAML required)")
        print("✓ Environment inspected")
        print("✓ Code patch applied (simulating LLM agent)")
        print("✓ Tests executed and passed")
        print("✓ Fix report generated")
        print("\nThis demonstrates the complete workflow for:")
        print("- Automated bug fixing with LLM agents")
        print("- Code repair in SWE-bench environments")
        print("- WarmPool management without Kubernetes knowledge")
        print("- Patch application and verification")
        print("- Test execution and validation")


if __name__ == "__main__":
    main()
