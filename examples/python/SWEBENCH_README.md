# SWE-bench Scenario Example

This example demonstrates using the ARL framework for SWE-bench (Software Engineering Benchmark) scenarios, where LLM agents automatically fix bugs in real-world codebases.

## Overview

The SWE-bench scenario shows a complete workflow for:
1. **Environment Setup**: Using pre-configured SWE-bench Docker images
2. **Code Inspection**: Examining the codebase and test structure
3. **Patch Application**: Applying fixes generated by LLM agents
4. **Test Execution**: Running test suites to verify the fixes
5. **Result Validation**: Confirming that the bug is fixed

## SWE-bench Image

This example uses the image:
```
swebench/swesmith.x86_64.emotion_1776_js-emotion.b882bcba
```

This is a SWE-bench instance for the [emotion](https://github.com/emotion-js/emotion) JavaScript library, containing a reproducible bug and test suite.

## Prerequisites

1. **Kubernetes cluster** with ARL operator deployed
2. **WarmPool created** with the SWE-bench image:
   ```bash
   kubectl apply -f /home/runner/work/agent-env/agent-env/config/samples/warmpool-swebench.yaml
   ```
3. **Verify WarmPool is ready**:
   ```bash
   kubectl get warmpool swebench-emotion
   kubectl get pods -l warmpool=swebench-emotion
   ```

## Running the Example

```bash
cd examples/python
uv run python 08_swebench_scenario.py
```

## What the Example Does

### 1. Environment Inspection
Checks the SWE-bench testbed directory structure and verifies the repository state.

### 2. Patch Application
Simulates an LLM agent applying a code fix:
- Creates a patch file with the fix
- Shows the original code
- Applies the patch to fix the bug
- Verifies the patched code

### 3. Test Execution
Runs the test suite to verify the fix:
- Creates a test runner script
- Executes all relevant tests
- Reports test results (pass/fail)

### 4. Report Generation
Creates a markdown report documenting:
- The issue that was fixed
- Root cause analysis
- Fix applied
- Test results
- Verification status

## Example Output

```
============================================================
Example: SWE-bench Scenario - Code Repair and Testing
============================================================

✓ Sandbox allocated from pool 'swebench-emotion'

[Step 1] Inspecting SWE-bench environment...
Environment check: Succeeded
Workspace:
/testbed
...

[Step 2] Applying code patch (simulating LLM agent fix)...
Patch application: Succeeded
...

[Step 3] Running test scripts to verify fix...
Test execution: Succeeded
Test results:
✓ Test 1: Patch applied correctly
✓ Test 2: Unit tests passed
✓ Test 3: Integration tests passed
✓ Test 4: Style application works correctly

[Step 4] Generating fix report...
Report generation: Succeeded

============================================================
SWE-bench Scenario Completed Successfully!
============================================================

✓ Environment inspected
✓ Code patch applied (simulating LLM agent)
✓ Tests executed and passed
✓ Fix report generated
```

## Use Cases

This pattern is ideal for:

- **Automated Bug Fixing**: LLM agents that generate and test code fixes
- **CI/CD Integration**: Automated testing of proposed fixes
- **Research Benchmarking**: Evaluating LLM code repair capabilities
- **Code Repair Systems**: Building automated software maintenance tools

## Integration with LLM Agents

In a real LLM agent system, you would:

1. **Analyze the bug**: Feed the issue description and code to the LLM
2. **Generate patch**: LLM produces a code fix
3. **Apply patch**: Use FilePatch steps to apply the generated fix
4. **Run tests**: Execute the test suite via Command steps
5. **Iterate**: If tests fail, regenerate the patch and retry
6. **Report**: Generate a summary of the fix and validation

## Architecture Benefits

Using ARL for SWE-bench scenarios provides:

- **Isolation**: Each bug fix runs in a clean environment
- **Reproducibility**: SWE-bench images ensure consistent environments
- **Scalability**: Warm pools enable parallel bug fixing
- **Low Latency**: Pre-warmed pods eliminate startup time
- **Kubernetes Native**: Leverages K8s for orchestration and resource management

## Advanced Scenarios

### Multiple Test Iterations
Reuse the sandbox to iterate on fixes:

```python
with SandboxSession(pool_ref="swebench-emotion", keep_alive=True) as session:
    for attempt in range(max_attempts):
        # Apply patch
        session.execute([...])
        
        # Run tests
        result = session.execute([...])
        
        if test_passed(result):
            break
```

### Parallel Bug Fixing
Process multiple bugs concurrently:

```python
from concurrent.futures import ThreadPoolExecutor

def fix_bug(bug_id):
    with SandboxSession(pool_ref="swebench-emotion") as session:
        # Fix workflow...
        pass

with ThreadPoolExecutor(max_workers=5) as executor:
    executor.map(fix_bug, bug_ids)
```

## Related Examples

- `01_basic_execution.py` - Basic task execution
- `07_sandbox_reuse.py` - Sandbox reuse patterns
- `05_error_handling.py` - Error handling strategies

## References

- [SWE-bench Paper](https://www.swebench.com/)
- [SWE-bench Repository](https://github.com/princeton-nlp/SWE-bench)
- [Emotion Library](https://github.com/emotion-js/emotion)
