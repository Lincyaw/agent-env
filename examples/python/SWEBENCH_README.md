# SWE-bench Scenario Example

This example demonstrates using the ARL framework for SWE-bench (Software Engineering Benchmark) scenarios, where LLM agents automatically fix bugs in real-world codebases.

## Overview

The SWE-bench scenario shows a complete workflow for:
1. **WarmPool Setup**: Creating warm pools programmatically via Python SDK (no YAML required!)
2. **Environment Setup**: Using pre-configured SWE-bench Docker images
3. **Code Inspection**: Examining the codebase and test structure
4. **Patch Application**: Applying fixes generated by LLM agents
5. **Test Execution**: Running test suites to verify the fixes
6. **Result Validation**: Confirming that the bug is fixed

## SWE-bench Image

This example uses the image:
```
swebench/swesmith.x86_64.emotion_1776_js-emotion.b882bcba
```

This is a SWE-bench instance for the [emotion](https://github.com/emotion-js/emotion) JavaScript library, containing a reproducible bug and test suite.

## Prerequisites

1. **Kubernetes cluster** with ARL operator deployed
2. **Python SDK installed**:
   ```bash
   cd examples/python
   uv sync
   ```

That's it! No manual YAML configuration needed - the example creates the WarmPool programmatically.

## Running the Example

```bash
cd examples/python
uv run python 08_swebench_scenario.py
```

The example will:
- Automatically create the `swebench-emotion` WarmPool if it doesn't exist
- Wait for warm pods to be ready
- Execute the complete bug-fixing workflow

## What the Example Does

### 0. WarmPool Setup (NEW!)
Creates the WarmPool using Python SDK - no kubectl or YAML files needed:

```python
from arl import WarmPoolManager

manager = WarmPoolManager(namespace="default")
manager.create_warmpool(
    name="swebench-emotion",
    image="swebench/swesmith.x86_64.emotion_1776_js-emotion.b882bcba",
    replicas=2,
    testbed_path="/testbed"
)
manager.wait_for_warmpool_ready("swebench-emotion")
```

### 1. Environment Inspection
Checks the SWE-bench testbed directory structure and verifies the repository state.

### 2. Patch Application
Simulates an LLM agent applying a code fix:
- Creates a patch file with the fix
- Shows the original code
- Applies the patch to fix the bug
- Verifies the patched code

### 3. Test Execution
Runs the test suite to verify the fix:
- Creates a test runner script
- Executes all relevant tests
- Reports test results (pass/fail)

### 4. Report Generation
Creates a markdown report documenting:
- The issue that was fixed
- Root cause analysis
- Fix applied
- Test results
- Verification status

## Example Output

```
============================================================
Example: SWE-bench Scenario - Code Repair and Testing
============================================================

✓ Sandbox allocated from pool 'swebench-emotion'

[Step 1] Inspecting SWE-bench environment...
Environment check: Succeeded
Workspace:
/testbed
...

[Step 2] Applying code patch (simulating LLM agent fix)...
Patch application: Succeeded
...

[Step 3] Running test scripts to verify fix...
Test execution: Succeeded
Test results:
✓ Test 1: Patch applied correctly
✓ Test 2: Unit tests passed
✓ Test 3: Integration tests passed
✓ Test 4: Style application works correctly

[Step 4] Generating fix report...
Report generation: Succeeded

============================================================
SWE-bench Scenario Completed Successfully!
============================================================

✓ Environment inspected
✓ Code patch applied (simulating LLM agent)
✓ Tests executed and passed
✓ Fix report generated
```

## Use Cases

This pattern is ideal for:

- **Automated Bug Fixing**: LLM agents that generate and test code fixes
- **CI/CD Integration**: Automated testing of proposed fixes
- **Research Benchmarking**: Evaluating LLM code repair capabilities
- **Code Repair Systems**: Building automated software maintenance tools

## Integration with LLM Agents

In a real LLM agent system, you would:

1. **Analyze the bug**: Feed the issue description and code to the LLM
2. **Generate patch**: LLM produces a code fix
3. **Apply patch**: Use FilePatch steps to apply the generated fix
4. **Run tests**: Execute the test suite via Command steps
5. **Iterate**: If tests fail, regenerate the patch and retry
6. **Report**: Generate a summary of the fix and validation

## Architecture Benefits

Using ARL for SWE-bench scenarios provides:

- **No Kubernetes Knowledge Required**: Python SDK handles all infrastructure details
- **Isolation**: Each bug fix runs in a clean environment
- **Reproducibility**: SWE-bench images ensure consistent environments
- **Scalability**: Warm pools enable parallel bug fixing
- **Low Latency**: Pre-warmed pods eliminate startup time
- **Simple API**: Create, use, and manage resources with Python code only

## Advanced Scenarios

### Multiple Test Iterations
Reuse the sandbox to iterate on fixes:

```python
with SandboxSession(pool_ref="swebench-emotion", keep_alive=True) as session:
    for attempt in range(max_attempts):
        # Apply patch
        session.execute([...])
        
        # Run tests
        result = session.execute([...])
        
        if test_passed(result):
            break
```

### Parallel Bug Fixing
Process multiple bugs concurrently:

```python
from concurrent.futures import ThreadPoolExecutor

def fix_bug(bug_id):
    with SandboxSession(pool_ref="swebench-emotion") as session:
        # Fix workflow...
        pass

with ThreadPoolExecutor(max_workers=5) as executor:
    executor.map(fix_bug, bug_ids)
```

## Related Examples

- `01_basic_execution.py` - Basic task execution
- `07_sandbox_reuse.py` - Sandbox reuse patterns
- `05_error_handling.py` - Error handling strategies

## References

- [SWE-bench Paper](https://www.swebench.com/)
- [SWE-bench Repository](https://github.com/princeton-nlp/SWE-bench)
- [Emotion Library](https://github.com/emotion-js/emotion)
