# SWE-bench Fixtures

This directory contains fixture files for simulating agent inputs and outputs in SWE-bench scenarios.

## Files

### `issue_description.txt`
Bug report describing the issue to be fixed. In a real scenario, this would be:
- Extracted from GitHub issues
- Generated by issue analysis tools
- Provided by users or monitoring systems

### `agent_patch.diff`
Code patch generated by an LLM agent to fix the issue. In a real scenario, this would be:
- Generated by an LLM analyzing the codebase
- Created through iterative agent-environment interaction
- Refined based on test results

### `test_script.sh`
Test script to verify the fix. In a real scenario, this would be:
- Generated based on existing test suites
- Created to specifically test the fix
- Extended with new test cases for edge cases

### `fix_report_template.md`
Report template documenting the fix. In a real scenario, this would be:
- Generated to document the solution
- Include analysis and verification details
- Used for PR descriptions or documentation

## Usage

These fixtures are used by:
- `08_swebench_scenario.py` - Loads fixtures with fallback to inline content
- `08_swebench_scenario_mock.py` - Uses mock agent to load all fixtures
- `mock_agent.py` - Mock agent module that simulates LLM behavior

## Customization

You can modify these fixtures to:
- Test different bug scenarios
- Simulate various agent behaviors
- Create custom workflows
- Test error handling

## Example

```python
from pathlib import Path

def load_fixture(filename: str) -> str:
    fixtures_dir = Path(__file__).parent / "swebench_fixtures"
    with open(fixtures_dir / filename) as f:
        return f.read()

# Load agent-generated patch
patch = load_fixture("agent_patch.diff")
```
